
==> Audit <==
|-----------|----------------------------|----------|---------------|---------|---------------------|---------------------|
|  Command  |            Args            | Profile  |     User      | Version |     Start Time      |      End Time       |
|-----------|----------------------------|----------|---------------|---------|---------------------|---------------------|
| start     |                            | minikube | ANDURIL\brett | v1.33.1 | 02 Aug 24 22:11 EDT | 02 Aug 24 22:13 EDT |
| dashboard |                            | minikube | ANDURIL\brett | v1.33.1 | 02 Aug 24 22:13 EDT |                     |
| service   | hello-node                 | minikube | ANDURIL\brett | v1.33.1 | 02 Aug 24 22:17 EDT |                     |
| addons    | list                       | minikube | ANDURIL\brett | v1.33.1 | 02 Aug 24 22:17 EDT | 02 Aug 24 22:17 EDT |
| addons    | enable metrics-server      | minikube | ANDURIL\brett | v1.33.1 | 02 Aug 24 22:17 EDT | 02 Aug 24 22:17 EDT |
| stop      |                            | minikube | ANDURIL\brett | v1.33.1 | 02 Aug 24 22:24 EDT | 02 Aug 24 22:24 EDT |
| start     |                            | minikube | ANDURIL\brett | v1.33.1 | 05 Aug 24 10:34 EDT | 05 Aug 24 10:34 EDT |
| service   | streamlit-dynamodb-service | minikube | ANDURIL\brett | v1.33.1 | 05 Aug 24 10:35 EDT |                     |
|-----------|----------------------------|----------|---------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/08/05 10:34:09
Running on machine: Anduril
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0805 10:34:09.044527   27184 out.go:291] Setting OutFile to fd 104 ...
I0805 10:34:09.045040   27184 out.go:343] isatty.IsTerminal(104) = true
I0805 10:34:09.045040   27184 out.go:304] Setting ErrFile to fd 108...
I0805 10:34:09.045040   27184 out.go:343] isatty.IsTerminal(108) = true
W0805 10:34:09.055251   27184 root.go:314] Error reading config file at C:\Users\brett\.minikube\config\config.json: open C:\Users\brett\.minikube\config\config.json: The system cannot find the file specified.
I0805 10:34:09.063870   27184 out.go:298] Setting JSON to false
I0805 10:34:09.068982   27184 start.go:129] hostinfo: {"hostname":"Anduril","uptime":2270848,"bootTime":1720597600,"procs":355,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3880 Build 22631.3880","kernelVersion":"10.0.22631.3880 Build 22631.3880","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"3fa5ad04-c242-492a-8614-a48808c48b21"}
W0805 10:34:09.068982   27184 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0805 10:34:09.070006   27184 out.go:177] 😄  minikube v1.33.1 on Microsoft Windows 11 Home 10.0.22631.3880 Build 22631.3880
I0805 10:34:09.071544   27184 notify.go:220] Checking for updates...
I0805 10:34:09.075137   27184 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0805 10:34:09.076167   27184 driver.go:392] Setting default libvirt URI to qemu:///system
I0805 10:34:09.238886   27184 docker.go:122] docker version: linux-23.0.5:Docker Desktop 4.19.0 (106363)
I0805 10:34:09.243484   27184 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0805 10:34:09.423974   27184 info.go:266] docker info: {ID:2fbe3e0f-17f5-4fe0-8cc4-63e66e847b35 Containers:61 ContainersRunning:2 ContainersPaused:0 ContainersStopped:59 Images:49 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:476 OomKillDisable:true NGoroutines:928 SystemTime:2024-08-05 14:34:09.391213917 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:8208637952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:23.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID:v1.1.5-0-gf19387a Expected:v1.1.5-0-gf19387a} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.3] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.10.0]] Warnings:<nil>}}
I0805 10:34:09.425532   27184 out.go:177] ✨  Using the docker driver based on existing profile
I0805 10:34:09.426080   27184 start.go:297] selected driver: docker
I0805 10:34:09.426080   27184 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\brett:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0805 10:34:09.426590   27184 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0805 10:34:09.436358   27184 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0805 10:34:09.577048   27184 info.go:266] docker info: {ID:2fbe3e0f-17f5-4fe0-8cc4-63e66e847b35 Containers:61 ContainersRunning:2 ContainersPaused:0 ContainersStopped:59 Images:49 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:476 OomKillDisable:true NGoroutines:928 SystemTime:2024-08-05 14:34:09.560737464 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:8208637952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:23.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID:v1.1.5-0-gf19387a Expected:v1.1.5-0-gf19387a} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.3] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.10.0]] Warnings:<nil>}}
I0805 10:34:09.605581   27184 cni.go:84] Creating CNI manager for ""
I0805 10:34:09.605581   27184 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0805 10:34:09.606100   27184 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\brett:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0805 10:34:09.606618   27184 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0805 10:34:09.607715   27184 cache.go:121] Beginning downloading kic base image for docker with docker
I0805 10:34:09.608220   27184 out.go:177] 🚜  Pulling base image v0.0.44 ...
I0805 10:34:09.609780   27184 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0805 10:34:09.609780   27184 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0805 10:34:09.609780   27184 preload.go:147] Found local preload: C:\Users\brett\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0805 10:34:09.609780   27184 cache.go:56] Caching tarball of preloaded images
I0805 10:34:09.610298   27184 preload.go:173] Found C:\Users\brett\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0805 10:34:09.610298   27184 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0805 10:34:09.610298   27184 profile.go:143] Saving config to C:\Users\brett\.minikube\profiles\minikube\config.json ...
I0805 10:34:09.715230   27184 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0805 10:34:09.715230   27184 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0805 10:34:09.715853   27184 cache.go:194] Successfully downloaded all kic artifacts
I0805 10:34:09.716355   27184 start.go:360] acquireMachinesLock for minikube: {Name:mk84ebf9fa1e90ddc95a8064ade55747be92c868 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0805 10:34:09.716355   27184 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0805 10:34:09.716355   27184 start.go:96] Skipping create...Using existing machine configuration
I0805 10:34:09.716355   27184 fix.go:54] fixHost starting: 
I0805 10:34:09.726095   27184 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0805 10:34:09.808189   27184 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0805 10:34:09.808189   27184 fix.go:138] unexpected machine state, will restart: <nil>
I0805 10:34:09.808698   27184 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0805 10:34:09.814300   27184 cli_runner.go:164] Run: docker start minikube
I0805 10:34:10.294977   27184 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0805 10:34:10.381964   27184 kic.go:430] container "minikube" state is running.
I0805 10:34:10.388153   27184 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0805 10:34:10.473332   27184 profile.go:143] Saving config to C:\Users\brett\.minikube\profiles\minikube\config.json ...
I0805 10:34:10.474357   27184 machine.go:94] provisionDockerMachine start ...
I0805 10:34:10.479486   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:10.570242   27184 main.go:141] libmachine: Using SSH client type: native
I0805 10:34:10.576478   27184 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6fa3c0] 0x6fcfa0 <nil>  [] 0s} 127.0.0.1 59674 <nil> <nil>}
I0805 10:34:10.576478   27184 main.go:141] libmachine: About to run SSH command:
hostname
I0805 10:34:10.587362   27184 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0805 10:34:13.754994   27184 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0805 10:34:13.755526   27184 ubuntu.go:169] provisioning hostname "minikube"
I0805 10:34:13.761195   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:13.848754   27184 main.go:141] libmachine: Using SSH client type: native
I0805 10:34:13.848754   27184 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6fa3c0] 0x6fcfa0 <nil>  [] 0s} 127.0.0.1 59674 <nil> <nil>}
I0805 10:34:13.848754   27184 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0805 10:34:13.986957   27184 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0805 10:34:13.992126   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:14.078186   27184 main.go:141] libmachine: Using SSH client type: native
I0805 10:34:14.078699   27184 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6fa3c0] 0x6fcfa0 <nil>  [] 0s} 127.0.0.1 59674 <nil> <nil>}
I0805 10:34:14.078699   27184 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0805 10:34:14.222358   27184 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0805 10:34:14.222885   27184 ubuntu.go:175] set auth options {CertDir:C:\Users\brett\.minikube CaCertPath:C:\Users\brett\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\brett\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\brett\.minikube\machines\server.pem ServerKeyPath:C:\Users\brett\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\brett\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\brett\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\brett\.minikube}
I0805 10:34:14.222885   27184 ubuntu.go:177] setting up certificates
I0805 10:34:14.222885   27184 provision.go:84] configureAuth start
I0805 10:34:14.228573   27184 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0805 10:34:14.305941   27184 provision.go:143] copyHostCerts
I0805 10:34:14.310547   27184 exec_runner.go:144] found C:\Users\brett\.minikube/ca.pem, removing ...
I0805 10:34:14.310547   27184 exec_runner.go:203] rm: C:\Users\brett\.minikube\ca.pem
I0805 10:34:14.311060   27184 exec_runner.go:151] cp: C:\Users\brett\.minikube\certs\ca.pem --> C:\Users\brett\.minikube/ca.pem (1074 bytes)
I0805 10:34:14.314865   27184 exec_runner.go:144] found C:\Users\brett\.minikube/cert.pem, removing ...
I0805 10:34:14.314865   27184 exec_runner.go:203] rm: C:\Users\brett\.minikube\cert.pem
I0805 10:34:14.315381   27184 exec_runner.go:151] cp: C:\Users\brett\.minikube\certs\cert.pem --> C:\Users\brett\.minikube/cert.pem (1119 bytes)
I0805 10:34:14.319511   27184 exec_runner.go:144] found C:\Users\brett\.minikube/key.pem, removing ...
I0805 10:34:14.319511   27184 exec_runner.go:203] rm: C:\Users\brett\.minikube\key.pem
I0805 10:34:14.320025   27184 exec_runner.go:151] cp: C:\Users\brett\.minikube\certs\key.pem --> C:\Users\brett\.minikube/key.pem (1675 bytes)
I0805 10:34:14.320025   27184 provision.go:117] generating server cert: C:\Users\brett\.minikube\machines\server.pem ca-key=C:\Users\brett\.minikube\certs\ca.pem private-key=C:\Users\brett\.minikube\certs\ca-key.pem org=brett.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0805 10:34:14.430392   27184 provision.go:177] copyRemoteCerts
I0805 10:34:14.438442   27184 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0805 10:34:14.443600   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:14.522038   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:14.611897   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0805 10:34:14.628322   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0805 10:34:14.643367   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0805 10:34:14.657866   27184 provision.go:87] duration metric: took 434.9808ms to configureAuth
I0805 10:34:14.657866   27184 ubuntu.go:193] setting minikube options for container-runtime
I0805 10:34:14.657866   27184 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0805 10:34:14.662992   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:14.755335   27184 main.go:141] libmachine: Using SSH client type: native
I0805 10:34:14.755335   27184 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6fa3c0] 0x6fcfa0 <nil>  [] 0s} 127.0.0.1 59674 <nil> <nil>}
I0805 10:34:14.755335   27184 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0805 10:34:14.880766   27184 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0805 10:34:14.881307   27184 ubuntu.go:71] root file system type: overlay
I0805 10:34:14.881307   27184 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0805 10:34:14.886987   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:14.970934   27184 main.go:141] libmachine: Using SSH client type: native
I0805 10:34:14.971445   27184 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6fa3c0] 0x6fcfa0 <nil>  [] 0s} 127.0.0.1 59674 <nil> <nil>}
I0805 10:34:14.971445   27184 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0805 10:34:15.103698   27184 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0805 10:34:15.108815   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:15.185042   27184 main.go:141] libmachine: Using SSH client type: native
I0805 10:34:15.185553   27184 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6fa3c0] 0x6fcfa0 <nil>  [] 0s} 127.0.0.1 59674 <nil> <nil>}
I0805 10:34:15.185553   27184 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0805 10:34:15.325907   27184 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0805 10:34:15.325907   27184 machine.go:97] duration metric: took 4.8515492s to provisionDockerMachine
I0805 10:34:15.325907   27184 start.go:293] postStartSetup for "minikube" (driver="docker")
I0805 10:34:15.325907   27184 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0805 10:34:15.333600   27184 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0805 10:34:15.338760   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:15.415997   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:15.522048   27184 ssh_runner.go:195] Run: cat /etc/os-release
I0805 10:34:15.524632   27184 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0805 10:34:15.524632   27184 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0805 10:34:15.524632   27184 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0805 10:34:15.524632   27184 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0805 10:34:15.525153   27184 filesync.go:126] Scanning C:\Users\brett\.minikube\addons for local assets ...
I0805 10:34:15.525153   27184 filesync.go:126] Scanning C:\Users\brett\.minikube\files for local assets ...
I0805 10:34:15.525153   27184 start.go:296] duration metric: took 199.2464ms for postStartSetup
I0805 10:34:15.532454   27184 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0805 10:34:15.537611   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:15.616075   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:15.720101   27184 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0805 10:34:15.723390   27184 fix.go:56] duration metric: took 6.0070349s for fixHost
I0805 10:34:15.723390   27184 start.go:83] releasing machines lock for "minikube", held for 6.0070349s
I0805 10:34:15.728516   27184 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0805 10:34:15.804456   27184 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0805 10:34:15.810617   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:15.811642   27184 ssh_runner.go:195] Run: cat /version.json
I0805 10:34:15.817530   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:15.895426   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:15.911177   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:16.000129   27184 ssh_runner.go:195] Run: systemctl --version
I0805 10:34:16.254805   27184 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0805 10:34:16.265668   27184 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0805 10:34:16.271320   27184 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0805 10:34:16.279101   27184 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0805 10:34:16.284219   27184 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0805 10:34:16.284219   27184 start.go:494] detecting cgroup driver to use...
I0805 10:34:16.284219   27184 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0805 10:34:16.284733   27184 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0805 10:34:16.301216   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0805 10:34:16.314009   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0805 10:34:16.319130   27184 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0805 10:34:16.326353   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0805 10:34:16.339148   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0805 10:34:16.351492   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0805 10:34:16.363770   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0805 10:34:16.376706   27184 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0805 10:34:16.389103   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0805 10:34:16.402456   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0805 10:34:16.414737   27184 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0805 10:34:16.428054   27184 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0805 10:34:16.440423   27184 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0805 10:34:16.453279   27184 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0805 10:34:16.532229   27184 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0805 10:34:16.609450   27184 start.go:494] detecting cgroup driver to use...
I0805 10:34:16.609450   27184 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0805 10:34:16.617819   27184 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0805 10:34:16.625592   27184 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0805 10:34:16.634633   27184 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0805 10:34:16.642408   27184 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0805 10:34:16.660635   27184 ssh_runner.go:195] Run: which cri-dockerd
I0805 10:34:16.672143   27184 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0805 10:34:16.677807   27184 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0805 10:34:16.697414   27184 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0805 10:34:16.787538   27184 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0805 10:34:16.877335   27184 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0805 10:34:16.877335   27184 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0805 10:34:16.895055   27184 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0805 10:34:16.959335   27184 ssh_runner.go:195] Run: sudo systemctl restart docker
I0805 10:34:17.146554   27184 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0805 10:34:17.160003   27184 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0805 10:34:17.174137   27184 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0805 10:34:17.187302   27184 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0805 10:34:17.284580   27184 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0805 10:34:17.359227   27184 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0805 10:34:17.440419   27184 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0805 10:34:17.455795   27184 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0805 10:34:17.469357   27184 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0805 10:34:17.550482   27184 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0805 10:34:17.688937   27184 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0805 10:34:17.696696   27184 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0805 10:34:17.699261   27184 start.go:562] Will wait 60s for crictl version
I0805 10:34:17.706949   27184 ssh_runner.go:195] Run: which crictl
I0805 10:34:17.716777   27184 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0805 10:34:17.812425   27184 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0805 10:34:17.817533   27184 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0805 10:34:17.895518   27184 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0805 10:34:17.911259   27184 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0805 10:34:17.916909   27184 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0805 10:34:18.053508   27184 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0805 10:34:18.061164   27184 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0805 10:34:18.064249   27184 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0805 10:34:18.075562   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0805 10:34:18.163906   27184 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\brett:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0805 10:34:18.163906   27184 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0805 10:34:18.169005   27184 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0805 10:34:18.185924   27184 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0805 10:34:18.185924   27184 docker.go:615] Images already preloaded, skipping extraction
I0805 10:34:18.191045   27184 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0805 10:34:18.203940   27184 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0805 10:34:18.203940   27184 cache_images.go:84] Images are preloaded, skipping loading
I0805 10:34:18.203940   27184 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0805 10:34:18.204490   27184 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0805 10:34:18.209687   27184 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0805 10:34:18.327114   27184 cni.go:84] Creating CNI manager for ""
I0805 10:34:18.327114   27184 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0805 10:34:18.327114   27184 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0805 10:34:18.327114   27184 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0805 10:34:18.327114   27184 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0805 10:34:18.334922   27184 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0805 10:34:18.341134   27184 binaries.go:44] Found k8s binaries, skipping transfer
I0805 10:34:18.348422   27184 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0805 10:34:18.353603   27184 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0805 10:34:18.362605   27184 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0805 10:34:18.373165   27184 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0805 10:34:18.391497   27184 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0805 10:34:18.394086   27184 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0805 10:34:18.407618   27184 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0805 10:34:18.486835   27184 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0805 10:34:18.496624   27184 certs.go:68] Setting up C:\Users\brett\.minikube\profiles\minikube for IP: 192.168.49.2
I0805 10:34:18.496624   27184 certs.go:194] generating shared ca certs ...
I0805 10:34:18.496624   27184 certs.go:226] acquiring lock for ca certs: {Name:mkd973dc51c3cf006d9223607a339e767ec423ed Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0805 10:34:18.505934   27184 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\brett\.minikube\ca.key
I0805 10:34:18.513173   27184 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\brett\.minikube\proxy-client-ca.key
I0805 10:34:18.513173   27184 certs.go:256] generating profile certs ...
I0805 10:34:18.513173   27184 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\brett\.minikube\profiles\minikube\client.key
I0805 10:34:18.520913   27184 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\brett\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0805 10:34:18.526999   27184 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\brett\.minikube\profiles\minikube\proxy-client.key
I0805 10:34:18.529000   27184 certs.go:484] found cert: C:\Users\brett\.minikube\certs\ca-key.pem (1675 bytes)
I0805 10:34:18.529000   27184 certs.go:484] found cert: C:\Users\brett\.minikube\certs\ca.pem (1074 bytes)
I0805 10:34:18.529000   27184 certs.go:484] found cert: C:\Users\brett\.minikube\certs\cert.pem (1119 bytes)
I0805 10:34:18.529000   27184 certs.go:484] found cert: C:\Users\brett\.minikube\certs\key.pem (1675 bytes)
I0805 10:34:18.531613   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0805 10:34:18.545804   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0805 10:34:18.558788   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0805 10:34:18.571200   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0805 10:34:18.584109   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0805 10:34:18.599357   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0805 10:34:18.613404   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0805 10:34:18.626828   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0805 10:34:18.640386   27184 ssh_runner.go:362] scp C:\Users\brett\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0805 10:34:18.653382   27184 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0805 10:34:18.674904   27184 ssh_runner.go:195] Run: openssl version
I0805 10:34:18.708071   27184 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0805 10:34:18.723122   27184 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0805 10:34:18.726231   27184 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug  3 02:12 /usr/share/ca-certificates/minikubeCA.pem
I0805 10:34:18.734016   27184 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0805 10:34:18.746431   27184 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0805 10:34:18.759886   27184 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0805 10:34:18.770217   27184 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0805 10:34:18.782654   27184 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0805 10:34:18.794503   27184 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0805 10:34:18.806936   27184 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0805 10:34:18.819400   27184 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0805 10:34:18.832395   27184 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0805 10:34:18.837064   27184 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\brett:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0805 10:34:18.841700   27184 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0805 10:34:18.860827   27184 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0805 10:34:18.865992   27184 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0805 10:34:18.865992   27184 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0805 10:34:18.865992   27184 kubeadm.go:587] restartPrimaryControlPlane start ...
I0805 10:34:18.873764   27184 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0805 10:34:18.880084   27184 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0805 10:34:18.885228   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0805 10:34:18.963521   27184 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\brett\.kube\config
I0805 10:34:18.963521   27184 kubeconfig.go:62] C:\Users\brett\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0805 10:34:18.964037   27184 lock.go:35] WriteFile acquiring C:\Users\brett\.kube\config: {Name:mkcef4a1da39ce9cc1c298a1315766bea1b37ced Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0805 10:34:18.976840   27184 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0805 10:34:18.982473   27184 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0805 10:34:18.982473   27184 kubeadm.go:591] duration metric: took 116.4812ms to restartPrimaryControlPlane
I0805 10:34:18.982473   27184 kubeadm.go:393] duration metric: took 145.4085ms to StartCluster
I0805 10:34:18.982473   27184 settings.go:142] acquiring lock: {Name:mk95b0dcba4b7b9fa6fd0a33d533f09f58952d01 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0805 10:34:18.982473   27184 settings.go:150] Updating kubeconfig:  C:\Users\brett\.kube\config
I0805 10:34:18.982990   27184 lock.go:35] WriteFile acquiring C:\Users\brett\.kube\config: {Name:mkcef4a1da39ce9cc1c298a1315766bea1b37ced Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0805 10:34:18.983508   27184 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0805 10:34:18.983508   27184 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0805 10:34:18.983508   27184 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0805 10:34:18.983508   27184 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0805 10:34:18.983508   27184 addons.go:69] Setting default-storageclass=true in profile "minikube"
W0805 10:34:18.983508   27184 addons.go:243] addon storage-provisioner should already be in state true
I0805 10:34:18.983508   27184 addons.go:69] Setting dashboard=true in profile "minikube"
I0805 10:34:18.983508   27184 addons.go:234] Setting addon dashboard=true in "minikube"
W0805 10:34:18.983508   27184 addons.go:243] addon dashboard should already be in state true
I0805 10:34:18.983508   27184 addons.go:69] Setting metrics-server=true in profile "minikube"
I0805 10:34:18.983508   27184 addons.go:234] Setting addon metrics-server=true in "minikube"
W0805 10:34:18.983508   27184 addons.go:243] addon metrics-server should already be in state true
I0805 10:34:18.983508   27184 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0805 10:34:18.984024   27184 out.go:177] 🔎  Verifying Kubernetes components...
I0805 10:34:18.984024   27184 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0805 10:34:18.984024   27184 host.go:66] Checking if "minikube" exists ...
I0805 10:34:18.984024   27184 host.go:66] Checking if "minikube" exists ...
I0805 10:34:18.984024   27184 host.go:66] Checking if "minikube" exists ...
I0805 10:34:18.998215   27184 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0805 10:34:19.000845   27184 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0805 10:34:19.001872   27184 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0805 10:34:19.001872   27184 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0805 10:34:19.001872   27184 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0805 10:34:19.079734   27184 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0805 10:34:19.092752   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0805 10:34:19.102607   27184 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0805 10:34:19.104158   27184 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0805 10:34:19.104678   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0805 10:34:19.104678   27184 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0805 10:34:19.109879   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:19.118775   27184 out.go:177]     ▪ Using image registry.k8s.io/metrics-server/metrics-server:v0.7.1
I0805 10:34:19.119294   27184 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0805 10:34:19.120339   27184 addons.go:426] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0805 10:34:19.120862   27184 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0805 10:34:19.120862   27184 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0805 10:34:19.120862   27184 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0805 10:34:19.128681   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:19.130255   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:19.133896   27184 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0805 10:34:19.133896   27184 addons.go:243] addon default-storageclass should already be in state true
I0805 10:34:19.134417   27184 host.go:66] Checking if "minikube" exists ...
I0805 10:34:19.148599   27184 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0805 10:34:19.194261   27184 api_server.go:52] waiting for apiserver process to appear ...
I0805 10:34:19.203666   27184 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0805 10:34:19.208929   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:19.224117   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:19.238738   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:19.254884   27184 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0805 10:34:19.254884   27184 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0805 10:34:19.259987   27184 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0805 10:34:19.308688   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0805 10:34:19.308688   27184 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0805 10:34:19.320100   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0805 10:34:19.320100   27184 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0805 10:34:19.331047   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0805 10:34:19.331047   27184 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0805 10:34:19.341523   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0805 10:34:19.341523   27184 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0805 10:34:19.346214   27184 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59674 SSHKeyPath:C:\Users\brett\.minikube\machines\minikube\id_rsa Username:docker}
I0805 10:34:19.351929   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-role.yaml
I0805 10:34:19.351929   27184 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0805 10:34:19.405778   27184 addons.go:426] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0805 10:34:19.405778   27184 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0805 10:34:19.411608   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0805 10:34:19.411608   27184 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0805 10:34:19.417834   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0805 10:34:19.417834   27184 addons.go:426] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0805 10:34:19.417834   27184 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0805 10:34:19.422041   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0805 10:34:19.422041   27184 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0805 10:34:19.427753   27184 addons.go:426] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0805 10:34:19.427753   27184 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0805 10:34:19.432412   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0805 10:34:19.432412   27184 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0805 10:34:19.447541   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0805 10:34:19.503171   27184 addons.go:426] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0805 10:34:19.503171   27184 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0805 10:34:19.523557   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0805 10:34:19.526207   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0805 10:34:19.728722   27184 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0805 10:34:20.001070   27184 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0805 10:34:20.001070   27184 retry.go:31] will retry after 330.342382ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0805 10:34:20.004065   27184 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0805 10:34:20.004065   27184 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0805 10:34:20.004065   27184 retry.go:31] will retry after 362.687058ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0805 10:34:20.004065   27184 retry.go:31] will retry after 364.288042ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0805 10:34:20.004065   27184 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0805 10:34:20.004065   27184 retry.go:31] will retry after 125.382072ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0805 10:34:20.004065   27184 api_server.go:72] duration metric: took 1.020557s to wait for apiserver process to appear ...
I0805 10:34:20.004065   27184 api_server.go:88] waiting for apiserver healthz status ...
I0805 10:34:20.004065   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:20.006190   27184 api_server.go:269] stopped: https://127.0.0.1:59678/healthz: Get "https://127.0.0.1:59678/healthz": EOF
I0805 10:34:20.140003   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0805 10:34:20.343584   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0805 10:34:20.374724   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0805 10:34:20.382006   27184 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0805 10:34:20.507700   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:21.921433   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0805 10:34:21.921968   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0805 10:34:21.921968   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:22.204606   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:22.204606   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:22.204606   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:22.210458   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:22.210458   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:22.505414   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:22.509447   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:22.509447   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:23.018573   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:23.022768   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:23.022768   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:23.336375   27184 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.9927903s)
I0805 10:34:23.336375   27184 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (3.1963712s)
I0805 10:34:23.336375   27184 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.9616501s)
I0805 10:34:23.336375   27184 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (2.9543688s)
I0805 10:34:23.336890   27184 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0805 10:34:23.336890   27184 addons.go:470] Verifying addon metrics-server=true in "minikube"
I0805 10:34:23.344147   27184 out.go:177] 🌟  Enabled addons: storage-provisioner, metrics-server, dashboard, default-storageclass
I0805 10:34:23.345200   27184 addons.go:505] duration metric: took 4.3616925s for enable addons: enabled=[storage-provisioner metrics-server dashboard default-storageclass]
I0805 10:34:23.517487   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:23.521526   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:23.522065   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:24.016015   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:24.025492   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:24.025492   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:24.514792   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:24.521801   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:24.522329   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:25.012774   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:25.016916   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:25.016916   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:25.514994   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:25.518115   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:25.518115   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:26.015521   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:26.019311   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:26.019311   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:26.516030   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:26.520143   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:26.520143   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:27.017668   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:27.020941   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0805 10:34:27.020941   27184 api_server.go:103] status: https://127.0.0.1:59678/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0805 10:34:27.515858   27184 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59678/healthz ...
I0805 10:34:27.529045   27184 api_server.go:279] https://127.0.0.1:59678/healthz returned 200:
ok
I0805 10:34:27.534346   27184 api_server.go:141] control plane version: v1.30.0
I0805 10:34:27.534346   27184 api_server.go:131] duration metric: took 7.5302806s to wait for apiserver health ...
I0805 10:34:27.534860   27184 system_pods.go:43] waiting for kube-system pods to appear ...
I0805 10:34:27.542227   27184 system_pods.go:59] 8 kube-system pods found
I0805 10:34:27.542227   27184 system_pods.go:61] "coredns-7db6d8ff4d-k9dkh" [7a11cf4d-62d9-419d-b636-3e462d318480] Running
I0805 10:34:27.542227   27184 system_pods.go:61] "etcd-minikube" [2c65ac95-1df1-4455-b04b-ce3c42483c6b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0805 10:34:27.542227   27184 system_pods.go:61] "kube-apiserver-minikube" [91e1c573-cae5-4f6f-8da6-594789e07015] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0805 10:34:27.542227   27184 system_pods.go:61] "kube-controller-manager-minikube" [eed69d3c-e484-4fd8-8975-7fe557de2593] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0805 10:34:27.542227   27184 system_pods.go:61] "kube-proxy-25kz8" [bf018aa9-0995-4afa-a141-2311ca1fd623] Running
I0805 10:34:27.542227   27184 system_pods.go:61] "kube-scheduler-minikube" [ce59ffd1-90a4-4ebf-a85a-038b6164e31c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0805 10:34:27.542227   27184 system_pods.go:61] "metrics-server-c59844bb4-rd756" [c4404b22-9db0-4208-a18e-927d70041ac7] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0805 10:34:27.542227   27184 system_pods.go:61] "storage-provisioner" [5a24710a-8c1e-4212-aebb-f4b7f7432dde] Running
I0805 10:34:27.542227   27184 system_pods.go:74] duration metric: took 7.367ms to wait for pod list to return data ...
I0805 10:34:27.542227   27184 kubeadm.go:576] duration metric: took 8.5587187s to wait for: map[apiserver:true system_pods:true]
I0805 10:34:27.542227   27184 node_conditions.go:102] verifying NodePressure condition ...
I0805 10:34:27.545298   27184 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0805 10:34:27.545298   27184 node_conditions.go:123] node cpu capacity is 20
I0805 10:34:27.545298   27184 node_conditions.go:105] duration metric: took 3.0709ms to run NodePressure ...
I0805 10:34:27.545298   27184 start.go:240] waiting for startup goroutines ...
I0805 10:34:27.545298   27184 start.go:245] waiting for cluster config update ...
I0805 10:34:27.545298   27184 start.go:254] writing updated cluster config ...
I0805 10:34:27.553556   27184 ssh_runner.go:195] Run: rm -f paused
I0805 10:34:27.645707   27184 start.go:600] kubectl: 1.25.9, cluster: 1.30.0 (minor skew: 5)
I0805 10:34:27.646214   27184 out.go:177] 
W0805 10:34:27.646759   27184 out.go:239] ❗  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.25.9, which may have incompatibilities with Kubernetes 1.30.0.
I0805 10:34:27.648307   27184 out.go:177]     ▪ Want kubectl v1.30.0? Try 'minikube kubectl -- get pods -A'
I0805 10:34:27.648835   27184 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 05 14:34:16 minikube dockerd[765]: time="2024-08-05T14:34:16.966625276Z" level=info msg="Daemon shutdown complete"
Aug 05 14:34:16 minikube dockerd[765]: time="2024-08-05T14:34:16.966792477Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Aug 05 14:34:16 minikube systemd[1]: docker.service: Deactivated successfully.
Aug 05 14:34:16 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 05 14:34:16 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 05 14:34:16 minikube dockerd[1005]: time="2024-08-05T14:34:16.995643332Z" level=info msg="Starting up"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.007845777Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.017378680Z" level=info msg="Loading containers: start."
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.084683028Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.103565496Z" level=info msg="Loading containers: done."
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.111596242Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.111616098Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.111619247Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.111621379Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.111632006Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.111658101Z" level=info msg="Daemon has completed initialization"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.137720436Z" level=info msg="API listen on /var/run/docker.sock"
Aug 05 14:34:17 minikube dockerd[1005]: time="2024-08-05T14:34:17.137762535Z" level=info msg="API listen on [::]:2376"
Aug 05 14:34:17 minikube systemd[1]: Started Docker Application Container Engine.
Aug 05 14:34:17 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Start docker client with request timeout 0s"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Loaded network plugin cni"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 05 14:34:17 minikube cri-dockerd[1254]: time="2024-08-05T14:34:17Z" level=info msg="Start cri-dockerd grpc backend"
Aug 05 14:34:17 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 05 14:34:18 minikube cri-dockerd[1254]: time="2024-08-05T14:34:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-b5fc48f67-mt69p_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5bc08cebe07b88260bd9a361ed6bea7b4b207188a7cdfb92a7e7971e92c557b7\""
Aug 05 14:34:18 minikube cri-dockerd[1254]: time="2024-08-05T14:34:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-779776cb65-rf66d_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ce8a24a898104c62177c35d57ed7527166f14417be513cf7c4cbfbae95bdeb5e\""
Aug 05 14:34:18 minikube cri-dockerd[1254]: time="2024-08-05T14:34:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"metrics-server-c59844bb4-rd756_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e52dd65f80e2ac92fa5450177fedd3523778d1152753fb4118ba467d70b497ab\""
Aug 05 14:34:18 minikube cri-dockerd[1254]: time="2024-08-05T14:34:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-k9dkh_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"adbd404db08f9d4cac0d841815d64c102037579749c01f4402d3229b7e304198\""
Aug 05 14:34:18 minikube cri-dockerd[1254]: time="2024-08-05T14:34:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-node-55fdcd95bf-pscv9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"40cc51c2121c823d750305fb67be31c05cb1826792f943fcc2b4b9bc2a00f715\""
Aug 05 14:34:19 minikube cri-dockerd[1254]: time="2024-08-05T14:34:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/182dff36ff334ce15835fd9720d66421486879c9934ca474e5471d187a62e39b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 14:34:19 minikube cri-dockerd[1254]: time="2024-08-05T14:34:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a86dace85eb70235d19f9e3f3fba69c81eec01393a7174fd81a65a485bdb657f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 14:34:19 minikube cri-dockerd[1254]: time="2024-08-05T14:34:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/90a1e5a5bd750d9462a26a4c1230ba99507dc39dcf6a9b74da610981a4a8cbb2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 14:34:19 minikube cri-dockerd[1254]: time="2024-08-05T14:34:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1cd26aafb70ffcff431cb683f3545eda037bdad1cb4128f53e5b627ff2f714d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 14:34:22 minikube cri-dockerd[1254]: time="2024-08-05T14:34:22Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 05 14:34:24 minikube cri-dockerd[1254]: time="2024-08-05T14:34:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/34787c1c3ee5fa84a0853dd44789f1d640720b28aeb3f2e5c660cbbb12280cda/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 14:34:24 minikube cri-dockerd[1254]: time="2024-08-05T14:34:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a8f69c87449ef1b30f633b100ac50e1d6dcd9c035f4ac924b53e6c23fbddeaef/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 14:34:24 minikube cri-dockerd[1254]: time="2024-08-05T14:34:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dd3ebb1a4b975a611ff99ffd2ef13c56c8bf87d9fe472edc95fa82389e48c98b/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 14:34:24 minikube cri-dockerd[1254]: time="2024-08-05T14:34:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dbfa6efc227aae4c88c4c9589e74aa40b018ab1a299951736a9f1903e5b7024d/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 14:34:24 minikube cri-dockerd[1254]: time="2024-08-05T14:34:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/40bef01fd9a44a5c95c789e33ecf0f3bdb8f2deb7198638782d2181f50de65ef/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 14:34:24 minikube cri-dockerd[1254]: time="2024-08-05T14:34:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/07195539f32807a973216bc34363a062600ca5485a461808d183419ea98816ba/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 14:34:35 minikube dockerd[1005]: time="2024-08-05T14:34:35.036154107Z" level=info msg="ignoring event" container=da88bf295aa77196022537d4c26cd009844adaa10f480ec6d6b11d835ab8f2a6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 05 14:34:35 minikube dockerd[1005]: time="2024-08-05T14:34:35.353498505Z" level=info msg="ignoring event" container=f73e5f493d288cc321a5a7f10974643d7780754f672b2ab183c11907e26119e8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 05 14:35:11 minikube cri-dockerd[1254]: time="2024-08-05T14:35:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6720b25d09b493b629a85501807a6a2c0d444be89383a042f64cb0f59cf2582b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 14:35:22 minikube cri-dockerd[1254]: time="2024-08-05T14:35:22Z" level=info msg="Pulling image amazon/dynamodb-local:latest: b510243a5cd9: Downloading [============================================>      ]  47.42MB/53.86MB"
Aug 05 14:35:32 minikube cri-dockerd[1254]: time="2024-08-05T14:35:32Z" level=info msg="Pulling image amazon/dynamodb-local:latest: 88169dbe8767: Downloading [========================================>          ]  107.8MB/133.7MB"
Aug 05 14:35:38 minikube cri-dockerd[1254]: time="2024-08-05T14:35:38Z" level=info msg="Stop pulling image amazon/dynamodb-local:latest: Status: Downloaded newer image for amazon/dynamodb-local:latest"
Aug 05 14:35:40 minikube cri-dockerd[1254]: time="2024-08-05T14:35:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d74b5cf5198a35a01d2a5827248daf1bfe58f6d9cce8bd446d2c3eeb7de9219b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 14:35:41 minikube dockerd[1005]: time="2024-08-05T14:35:41.203880504Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=64f1698c02f4d13f traceID=8f7b591d0768dfe6ba14ccd281fce1a4
Aug 05 14:35:41 minikube dockerd[1005]: time="2024-08-05T14:35:41.203951613Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 14:35:56 minikube dockerd[1005]: time="2024-08-05T14:35:56.380548817Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=640548001b245c2b traceID=461efb00682abd4ebc6dca1ea18a0bf9
Aug 05 14:35:56 minikube dockerd[1005]: time="2024-08-05T14:35:56.380679788Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 14:36:27 minikube dockerd[1005]: time="2024-08-05T14:36:27.465167799Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=905f54b077376f2c traceID=ffec6336d6e3e028bea9a9dca166b11a
Aug 05 14:36:27 minikube dockerd[1005]: time="2024-08-05T14:36:27.465386966Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                                   CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
be9cc82338b45       amazon/dynamodb-local@sha256:d7ebddeb60fa418bcda218a6c6a402a58441b2a20d54c9cb1d85fd5194341753                           About a minute ago   Running             dynamodb-local              0                   6720b25d09b49       dynamodb-local-57d4f8ccd9-t9x56
774a9b74dbfaa       07655ddf2eebe                                                                                                           2 minutes ago        Running             kubernetes-dashboard        2                   dd3ebb1a4b975       kubernetes-dashboard-779776cb65-rf66d
a66d1a519d720       6e38f40d628db                                                                                                           2 minutes ago        Running             storage-provisioner         2                   a8f69c87449ef       storage-provisioner
9cc2918ee1f41       a24c7c057ec87                                                                                                           2 minutes ago        Running             metrics-server              1                   07195539f3280       metrics-server-c59844bb4-rd756
f73e5f493d288       07655ddf2eebe                                                                                                           2 minutes ago        Exited              kubernetes-dashboard        1                   dd3ebb1a4b975       kubernetes-dashboard-779776cb65-rf66d
e850566cb5c0e       cbb01a7bd410d                                                                                                           2 minutes ago        Running             coredns                     1                   40bef01fd9a44       coredns-7db6d8ff4d-k9dkh
fa15414c3bcb0       115053965e86b                                                                                                           2 minutes ago        Running             dashboard-metrics-scraper   1                   dbfa6efc227aa       dashboard-metrics-scraper-b5fc48f67-mt69p
da88bf295aa77       6e38f40d628db                                                                                                           2 minutes ago        Exited              storage-provisioner         1                   a8f69c87449ef       storage-provisioner
33ef4f95e1f46       a0bf559e280cf                                                                                                           2 minutes ago        Running             kube-proxy                  1                   34787c1c3ee5f       kube-proxy-25kz8
131dc94c41cfc       c42f13656d0b2                                                                                                           2 minutes ago        Running             kube-apiserver              1                   90a1e5a5bd750       kube-apiserver-minikube
f38968e7e097f       c7aad43836fa5                                                                                                           2 minutes ago        Running             kube-controller-manager     1                   d1cd26aafb70f       kube-controller-manager-minikube
2c551487b7453       259c8277fcbbc                                                                                                           2 minutes ago        Running             kube-scheduler              1                   a86dace85eb70       kube-scheduler-minikube
31c56bc7b6563       3861cfcd7c04c                                                                                                           2 minutes ago        Running             etcd                        1                   182dff36ff334       etcd-minikube
bb937198d7695       registry.k8s.io/metrics-server/metrics-server@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a   2 days ago           Exited              metrics-server              0                   e52dd65f80e2a       metrics-server-c59844bb4-rd756
25f1b38b6ff7e       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c                    2 days ago           Exited              dashboard-metrics-scraper   0                   5bc08cebe07b8       dashboard-metrics-scraper-b5fc48f67-mt69p
b8fffb465b2ac       cbb01a7bd410d                                                                                                           2 days ago           Exited              coredns                     0                   adbd404db08f9       coredns-7db6d8ff4d-k9dkh
4b4af4edffcf2       a0bf559e280cf                                                                                                           2 days ago           Exited              kube-proxy                  0                   bd66d5a5cb4a2       kube-proxy-25kz8
ffa3bc8e1ad73       259c8277fcbbc                                                                                                           2 days ago           Exited              kube-scheduler              0                   058a22e4f06b4       kube-scheduler-minikube
9f853d94dde39       3861cfcd7c04c                                                                                                           2 days ago           Exited              etcd                        0                   36c75bcaa587f       etcd-minikube
2d6f3a825fa1e       c7aad43836fa5                                                                                                           2 days ago           Exited              kube-controller-manager     0                   0ff4fb4a35e76       kube-controller-manager-minikube
a21e534b50228       c42f13656d0b2                                                                                                           2 days ago           Exited              kube-apiserver              0                   cf46eac33f234       kube-apiserver-minikube


==> coredns [b8fffb465b2a] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:49162 - 10178 "HINFO IN 5777931373015086645.9120684374137887801. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.073272022s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[823781613]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (03-Aug-2024 02:13:19.726) (total time: 10002ms):
Trace[823781613]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (02:13:29.728)
Trace[823781613]: [10.00290743s] [10.00290743s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[754855891]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (03-Aug-2024 02:13:19.726) (total time: 10002ms):
Trace[754855891]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (02:13:29.728)
Trace[754855891]: [10.002981072s] [10.002981072s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1561977464]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (03-Aug-2024 02:13:19.726) (total time: 10003ms):
Trace[1561977464]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (02:13:29.728)
Trace[1561977464]: [10.003075538s] [10.003075538s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [e850566cb5c0] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:58790 - 14030 "HINFO IN 4694586228366321024.5141407670826686651. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.077427318s
[INFO] 10.244.0.11:37555 - 28316 "AAAA IN cognito-identity.us-east-1.amazonaws.com.default.svc.cluster.local. udp 84 false 512" NXDOMAIN qr,aa,rd 177 0.000586778s
[INFO] 10.244.0.11:37555 - 28314 "A IN cognito-identity.us-east-1.amazonaws.com.default.svc.cluster.local. udp 84 false 512" NXDOMAIN qr,aa,rd 177 0.000610086s
[INFO] 10.244.0.11:58583 - 12238 "AAAA IN cognito-identity.us-east-1.amazonaws.com.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.000082485s
[INFO] 10.244.0.11:58583 - 41676 "A IN cognito-identity.us-east-1.amazonaws.com.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.000118343s
[INFO] 10.244.0.11:38906 - 52936 "AAAA IN cognito-identity.us-east-1.amazonaws.com.cluster.local. udp 72 false 512" NXDOMAIN qr,aa,rd 165 0.000070545s
[INFO] 10.244.0.11:38906 - 59334 "A IN cognito-identity.us-east-1.amazonaws.com.cluster.local. udp 72 false 512" NXDOMAIN qr,aa,rd 165 0.000103471s
[INFO] 10.244.0.11:38933 - 25236 "AAAA IN cognito-identity.us-east-1.amazonaws.com. udp 58 false 512" NOERROR qr,rd,ra 602 0.039498883s
[INFO] 10.244.0.11:38933 - 4234 "A IN cognito-identity.us-east-1.amazonaws.com. udp 58 false 512" NOERROR qr,rd,ra 506 0.069348766s
[INFO] 10.244.0.11:54458 - 39589 "AAAA IN pinpoint.us-east-1.amazonaws.com.default.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.00011659s
[INFO] 10.244.0.11:54458 - 19620 "A IN pinpoint.us-east-1.amazonaws.com.default.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.000200254s
[INFO] 10.244.0.11:50513 - 26779 "AAAA IN pinpoint.us-east-1.amazonaws.com.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000095182s
[INFO] 10.244.0.11:50513 - 2148 "A IN pinpoint.us-east-1.amazonaws.com.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000137815s
[INFO] 10.244.0.11:44858 - 7521 "AAAA IN pinpoint.us-east-1.amazonaws.com.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.0000796s
[INFO] 10.244.0.11:44858 - 60000 "A IN pinpoint.us-east-1.amazonaws.com.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000100918s
[INFO] 10.244.0.11:38087 - 19108 "A IN pinpoint.us-east-1.amazonaws.com. udp 50 false 512" NOERROR qr,rd,ra 242 0.058548049s
[INFO] 10.244.0.11:38087 - 27562 "AAAA IN pinpoint.us-east-1.amazonaws.com. udp 50 false 512" NOERROR qr,rd,ra 166 0.097467829s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_08_02T22_13_06_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 03 Aug 2024 02:13:02 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 05 Aug 2024 14:37:05 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 05 Aug 2024 14:35:54 +0000   Sat, 03 Aug 2024 02:13:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 05 Aug 2024 14:35:54 +0000   Sat, 03 Aug 2024 02:13:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 05 Aug 2024 14:35:54 +0000   Sat, 03 Aug 2024 02:13:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 05 Aug 2024 14:35:54 +0000   Sat, 03 Aug 2024 02:13:02 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8016248Ki
  pods:               110
Allocatable:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8016248Ki
  pods:               110
System Info:
  Machine ID:                 53cef0c07a0d40e28656a458a77b5b00
  System UUID:                53cef0c07a0d40e28656a458a77b5b00
  Boot ID:                    f27b001a-5129-4230-a82a-f9c91e4cd7ce
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                              ------------  ----------  ---------------  -------------  ---
  default                     dynamodb-local-57d4f8ccd9-t9x56                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m2s
  default                     streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93s
  kube-system                 coredns-7db6d8ff4d-k9dkh                          100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     2d12h
  kube-system                 etcd-minikube                                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         2d12h
  kube-system                 kube-apiserver-minikube                           250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d12h
  kube-system                 kube-controller-manager-minikube                  200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d12h
  kube-system                 kube-proxy-25kz8                                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d12h
  kube-system                 kube-scheduler-minikube                           100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d12h
  kube-system                 metrics-server-c59844bb4-rd756                    100m (0%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         2d12h
  kube-system                 storage-provisioner                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d12h
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-mt69p         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d12h
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-rf66d             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d12h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (4%!)(MISSING)   0 (0%!)(MISSING)
  memory             370Mi (4%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 2d12h                  kube-proxy       
  Normal  Starting                 2m47s                  kube-proxy       
  Normal  NodeHasSufficientPID     2d12h                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  2d12h                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  2d12h                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2d12h                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 2d12h                  kubelet          Starting kubelet.
  Normal  RegisteredNode           2d12h                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 2m54s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  2m54s (x8 over 2m54s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m54s (x8 over 2m54s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m54s (x7 over 2m54s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  2m54s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           2m34s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Aug 3 15:01] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.004535] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.005894] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000013] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 3 15:02] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000109] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 3 16:22] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000139] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 3 16:23] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000894] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000318] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000803] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 3 18:09] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000097] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 3 19:10] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000000] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 00:12] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.005374] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 00:13] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.905745] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000462] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000246] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 04:04] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000180] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.010326] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000311] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 08:10] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000043] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 09:19] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000371] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[ +17.993195] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000063] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 11:44] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.003271] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 15:31] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.015758] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[ +19.723162] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000678] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000609] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000278] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 22:00] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000270] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.007801] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000092] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 22:12] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  -0.000007] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 4 23:33] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000011] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 5 02:16] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.003557] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 5 02:17] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.881739] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000366] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000073] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 5 12:18] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.862678] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000348] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000009] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[Aug 5 12:52] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.000101] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?


==> etcd [31c56bc7b656] <==
{"level":"warn","ts":"2024-08-05T14:34:20.008839Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-05T14:34:20.009352Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-08-05T14:34:20.009407Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-08-05T14:34:20.009428Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-05T14:34:20.009434Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-08-05T14:34:20.009446Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-05T14:34:20.013171Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-08-05T14:34:20.013475Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":20,"max-cpu-available":20,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-08-05T14:34:20.023288Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"7.755159ms"}
{"level":"info","ts":"2024-08-05T14:34:20.028304Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-08-05T14:34:20.031935Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":1252}
{"level":"info","ts":"2024-08-05T14:34:20.03199Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-08-05T14:34:20.032002Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2024-08-05T14:34:20.032007Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 1252, applied: 0, lastindex: 1252, lastterm: 2]"}
{"level":"warn","ts":"2024-08-05T14:34:20.03372Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-08-05T14:34:20.035181Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":742}
{"level":"info","ts":"2024-08-05T14:34:20.037025Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1096}
{"level":"info","ts":"2024-08-05T14:34:20.039009Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-08-05T14:34:20.041272Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-08-05T14:34:20.041442Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-08-05T14:34:20.041468Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-08-05T14:34:20.041567Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-08-05T14:34:20.041627Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-05T14:34:20.041733Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-05T14:34:20.041776Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-05T14:34:20.042251Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-08-05T14:34:20.042315Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-08-05T14:34:20.042391Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-05T14:34:20.042414Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-05T14:34:20.044327Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-05T14:34:20.044433Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-05T14:34:20.044462Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-05T14:34:20.044478Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-08-05T14:34:20.044491Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-08-05T14:34:21.132337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2024-08-05T14:34:21.133055Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2024-08-05T14:34:21.133164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-08-05T14:34:21.133202Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-08-05T14:34:21.133218Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-08-05T14:34:21.133242Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-08-05T14:34:21.133264Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-08-05T14:34:21.139589Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-08-05T14:34:21.139602Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-05T14:34:21.139606Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-05T14:34:21.139912Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-08-05T14:34:21.139951Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-08-05T14:34:21.141589Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-08-05T14:34:21.142131Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> etcd [9f853d94dde3] <==
{"level":"info","ts":"2024-08-03T02:13:01.372651Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-08-03T02:13:01.372744Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-03T02:13:01.372784Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-08-03T02:13:01.372825Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-03T02:13:01.373677Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-08-03T02:13:01.3738Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":20,"max-cpu-available":20,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-08-03T02:13:01.379897Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"5.648628ms"}
{"level":"info","ts":"2024-08-03T02:13:01.39284Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-08-03T02:13:01.392913Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-08-03T02:13:01.392928Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-08-03T02:13:01.392933Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-08-03T02:13:01.392936Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-08-03T02:13:01.392956Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-08-03T02:13:01.457046Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-08-03T02:13:01.462356Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-08-03T02:13:01.467215Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-08-03T02:13:01.472304Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-08-03T02:13:01.47248Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-08-03T02:13:01.472664Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-03T02:13:01.47287Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-03T02:13:01.47296Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-03T02:13:01.474053Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-03T02:13:01.474134Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-03T02:13:01.47417Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-03T02:13:01.474166Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-08-03T02:13:01.474189Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-08-03T02:13:01.475027Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-08-03T02:13:01.475122Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-08-03T02:13:01.69377Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-08-03T02:13:01.693819Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-08-03T02:13:01.693836Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-08-03T02:13:01.693847Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-08-03T02:13:01.69385Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-08-03T02:13:01.693855Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-08-03T02:13:01.693859Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-08-03T02:13:01.696525Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-03T02:13:01.698971Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-03T02:13:01.699001Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-03T02:13:01.698965Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-08-03T02:13:01.699149Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-08-03T02:13:01.699171Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-08-03T02:13:01.700069Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-08-03T02:13:01.700733Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-08-03T02:13:01.701344Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-03T02:13:01.701505Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-03T02:13:01.701557Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-03T02:23:01.856261Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":742}
{"level":"info","ts":"2024-08-03T02:23:01.86101Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":742,"took":"4.474395ms","hash":3749492190,"current-db-size-bytes":2191360,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":2191360,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-08-03T02:23:01.861048Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3749492190,"revision":742,"compact-revision":-1}
{"level":"info","ts":"2024-08-03T02:24:40.027905Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-08-03T02:24:40.028078Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-08-03T02:24:40.047064Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-03T02:24:40.047192Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2024/08/03 02:24:40 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-08-03T02:24:40.051933Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-03T02:24:40.051997Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-08-03T02:24:40.052074Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-08-03T02:24:40.056965Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-03T02:24:40.0573Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-03T02:24:40.057324Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 14:37:12 up 2 days,  3:59,  0 users,  load average: 0.14, 0.16, 0.07
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [131dc94c41cf] <==
I0805 14:34:22.197585       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0805 14:34:22.197677       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0805 14:34:22.197690       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0805 14:34:22.197737       1 shared_informer.go:320] Caches are synced for configmaps
I0805 14:34:22.197838       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0805 14:34:22.203218       1 shared_informer.go:320] Caches are synced for node_authorizer
I0805 14:34:22.205719       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0805 14:34:22.630469       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0805 14:34:22.729164       1 controller.go:615] quota admission added evaluator for: endpoints
I0805 14:34:22.919575       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0805 14:34:27.206085       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0805 14:34:27.214315       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
E0805 14:34:30.313975       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.116.71:443: connect: no route to host
E0805 14:34:32.209349       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: net/http: TLS handshake timeout
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0805 14:34:32.531018       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: a2a0e455-f548-46fe-a9a2-5ee3ef298e5d, UID in object meta: "
E0805 14:34:33.207488       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0805 14:34:33.207510       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0805 14:34:33.426092       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.101.116.71:443: connect: no route to host
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0805 14:34:33.426120       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0805 14:34:33.426105       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.116.71:443: connect: no route to host
W0805 14:34:33.426147       1 handler_proxy.go:93] no RequestInfo found in the context
E0805 14:34:33.426155       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0805 14:34:34.428859       1 handler_proxy.go:93] no RequestInfo found in the context
W0805 14:34:34.428928       1 handler_proxy.go:93] no RequestInfo found in the context
E0805 14:34:34.428964       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0805 14:34:34.428982       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0805 14:34:34.429010       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0805 14:34:34.430445       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0805 14:34:36.547109       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.116.71:443: connect: no route to host
I0805 14:34:38.515968       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0805 14:34:39.666888       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.116.71:443: connect: no route to host
W0805 14:34:39.676012       1 handler_proxy.go:93] no RequestInfo found in the context
E0805 14:34:39.676059       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0805 14:34:40.677020       1 handler_proxy.go:93] no RequestInfo found in the context
W0805 14:34:40.677048       1 handler_proxy.go:93] no RequestInfo found in the context
E0805 14:34:40.677067       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0805 14:34:40.677074       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0805 14:34:40.677083       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0805 14:34:40.678303       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0805 14:34:48.494070       1 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0805 14:35:10.676525       1 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0805 14:35:10.754510       1 controller.go:615] quota admission added evaluator for: replicasets.apps
W0805 14:35:16.725201       1 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0805 14:35:27.755000       1 handler_proxy.go:93] no RequestInfo found in the context
E0805 14:35:27.755047       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0805 14:35:27.795787       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.116.71:443: connect: connection refused
E0805 14:35:27.796139       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.116.71:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.116.71:443: connect: connection refused
I0805 14:35:27.827827       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0805 14:35:30.321899       1 alloc.go:330] "allocated clusterIPs" service="default/dynamodb-local" clusterIPs={"IPv4":"10.98.49.18"}
I0805 14:35:45.616604       1 alloc.go:330] "allocated clusterIPs" service="default/streamlit-dynamodb-service" clusterIPs={"IPv4":"10.97.146.179"}


==> kube-apiserver [a21e534b5022] <==
I0803 02:24:40.050488       1 controller.go:176] quota evaluator worker shutdown
I0803 02:24:40.050493       1 controller.go:176] quota evaluator worker shutdown
I0803 02:24:40.050492       1 controller.go:176] quota evaluator worker shutdown
I0803 02:24:40.050500       1 controller.go:176] quota evaluator worker shutdown
W0803 02:24:41.049114       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049137       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049146       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049114       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049127       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049194       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049205       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049208       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049220       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049248       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049252       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049252       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049255       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049282       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049305       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049321       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049349       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049373       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049385       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049391       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049392       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049283       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049361       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049415       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049428       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049434       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049436       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049435       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049441       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049349       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049452       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049456       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049357       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049488       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049496       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049501       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049507       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049510       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049522       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049522       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049528       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049452       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049557       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049523       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049559       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049572       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049596       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049624       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049628       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049630       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049658       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049669       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049666       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049656       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049752       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0803 02:24:41.049761       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [2d6f3a825fa1] <==
I0803 02:13:18.013805       1 shared_informer.go:320] Caches are synced for persistent volume
I0803 02:13:18.013814       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0803 02:13:18.013902       1 shared_informer.go:320] Caches are synced for service account
I0803 02:13:18.014055       1 shared_informer.go:320] Caches are synced for attach detach
I0803 02:13:18.020142       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0803 02:13:18.098878       1 shared_informer.go:320] Caches are synced for cronjob
I0803 02:13:18.108610       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0803 02:13:18.164240       1 shared_informer.go:320] Caches are synced for crt configmap
I0803 02:13:18.196718       1 shared_informer.go:320] Caches are synced for resource quota
I0803 02:13:18.213865       1 shared_informer.go:320] Caches are synced for deployment
I0803 02:13:18.220867       1 shared_informer.go:320] Caches are synced for resource quota
I0803 02:13:18.263019       1 shared_informer.go:320] Caches are synced for disruption
I0803 02:13:18.629138       1 shared_informer.go:320] Caches are synced for garbage collector
I0803 02:13:18.681338       1 shared_informer.go:320] Caches are synced for garbage collector
I0803 02:13:18.681420       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0803 02:13:19.121816       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="104.50661ms"
I0803 02:13:19.134025       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="12.151812ms"
I0803 02:13:19.134088       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="24.808µs"
I0803 02:13:20.736851       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="31.878µs"
I0803 02:13:26.668499       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="9.510108ms"
E0803 02:13:26.668527       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0803 02:13:26.671779       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="3.215311ms"
E0803 02:13:26.671846       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0803 02:13:26.675168       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="8.394228ms"
E0803 02:13:26.675195       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-779776cb65" failed with pods "kubernetes-dashboard-779776cb65-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0803 02:13:26.676169       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="1.938828ms"
E0803 02:13:26.676191       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0803 02:13:26.677725       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="2.515403ms"
E0803 02:13:26.677746       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-779776cb65" failed with pods "kubernetes-dashboard-779776cb65-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0803 02:13:26.693560       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="13.199212ms"
I0803 02:13:26.705372       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="11.772553ms"
I0803 02:13:26.705386       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="8.472838ms"
I0803 02:13:26.705435       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="31.3µs"
I0803 02:13:26.705519       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="27.396µs"
I0803 02:13:26.709775       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="4.550257ms"
I0803 02:13:26.709841       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="39.778µs"
I0803 02:13:26.711222       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="21.811µs"
I0803 02:13:37.818566       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="4.59096ms"
I0803 02:13:37.818763       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="31.183µs"
I0803 02:13:39.439023       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="5.165454ms"
I0803 02:13:39.439080       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="23.506µs"
I0803 02:13:40.870287       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="4.870205ms"
I0803 02:13:40.870414       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="25.253µs"
I0803 02:14:53.111056       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-node-55fdcd95bf" duration="9.644296ms"
I0803 02:14:53.115581       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-node-55fdcd95bf" duration="4.475188ms"
I0803 02:14:53.115714       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-node-55fdcd95bf" duration="56.823µs"
I0803 02:14:53.122462       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-node-55fdcd95bf" duration="18.874µs"
I0803 02:15:03.183173       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-node-55fdcd95bf" duration="3.910562ms"
I0803 02:15:03.183233       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-node-55fdcd95bf" duration="22.442µs"
I0803 02:17:59.390766       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="12.878647ms"
I0803 02:17:59.400934       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="10.115074ms"
I0803 02:17:59.401004       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="31.981µs"
I0803 02:18:05.951514       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="40.746µs"
E0803 02:18:18.256648       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0803 02:18:18.659729       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E0803 02:18:48.259526       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0803 02:18:48.663704       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0803 02:19:10.011062       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="4.787019ms"
I0803 02:19:10.011125       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="25.546µs"
I0803 02:24:33.563263       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-node-55fdcd95bf" duration="5.588µs"


==> kube-controller-manager [f38968e7e097] <==
I0805 14:34:38.072955       1 shared_informer.go:320] Caches are synced for cidrallocator
I0805 14:34:38.082143       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0805 14:34:38.084290       1 shared_informer.go:320] Caches are synced for ReplicationController
I0805 14:34:38.085566       1 shared_informer.go:320] Caches are synced for deployment
I0805 14:34:38.087101       1 shared_informer.go:320] Caches are synced for persistent volume
I0805 14:34:38.088525       1 shared_informer.go:320] Caches are synced for GC
I0805 14:34:38.095926       1 shared_informer.go:320] Caches are synced for service account
I0805 14:34:38.098331       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0805 14:34:38.104548       1 shared_informer.go:320] Caches are synced for namespace
I0805 14:34:38.105852       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0805 14:34:38.105938       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="43.649µs"
I0805 14:34:38.105965       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="39.088µs"
I0805 14:34:38.105990       1 shared_informer.go:320] Caches are synced for stateful set
I0805 14:34:38.107021       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0805 14:34:38.107056       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0805 14:34:38.107191       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0805 14:34:38.108217       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0805 14:34:38.108247       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0805 14:34:38.110538       1 shared_informer.go:320] Caches are synced for crt configmap
I0805 14:34:38.111681       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0805 14:34:38.111944       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0805 14:34:38.116558       1 shared_informer.go:320] Caches are synced for expand
I0805 14:34:38.184716       1 shared_informer.go:320] Caches are synced for cronjob
I0805 14:34:38.187021       1 shared_informer.go:320] Caches are synced for TTL after finished
I0805 14:34:38.213555       1 shared_informer.go:320] Caches are synced for job
I0805 14:34:38.215700       1 shared_informer.go:320] Caches are synced for attach detach
I0805 14:34:38.220174       1 shared_informer.go:320] Caches are synced for resource quota
I0805 14:34:38.294659       1 shared_informer.go:320] Caches are synced for resource quota
I0805 14:34:38.296818       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0805 14:34:38.419403       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="313.43411ms"
I0805 14:34:38.420496       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="41.413µs"
I0805 14:34:38.420848       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="314.941069ms"
I0805 14:34:38.420899       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="17.663µs"
I0805 14:34:38.725811       1 shared_informer.go:320] Caches are synced for garbage collector
I0805 14:34:38.744959       1 shared_informer.go:320] Caches are synced for garbage collector
I0805 14:34:38.744980       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0805 14:34:43.097954       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="26.623µs"
I0805 14:34:55.334839       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="5.652539ms"
I0805 14:34:55.334885       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="21.429µs"
E0805 14:35:08.223572       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0805 14:35:08.730392       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0805 14:35:10.772040       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dynamodb-local-57d4f8ccd9" duration="15.730338ms"
I0805 14:35:10.774503       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dynamodb-local-57d4f8ccd9" duration="2.421058ms"
I0805 14:35:10.774606       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dynamodb-local-57d4f8ccd9" duration="63.439µs"
I0805 14:35:10.781371       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dynamodb-local-57d4f8ccd9" duration="21.556µs"
I0805 14:35:10.787071       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dynamodb-local-57d4f8ccd9" duration="27.975µs"
I0805 14:35:27.749527       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="5.372822ms"
I0805 14:35:27.749599       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="35.038µs"
I0805 14:35:38.617545       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dynamodb-local-57d4f8ccd9" duration="8.584185ms"
I0805 14:35:38.617692       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dynamodb-local-57d4f8ccd9" duration="51.737µs"
I0805 14:35:39.617535       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="13.443563ms"
I0805 14:35:39.623596       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="5.934181ms"
I0805 14:35:39.623774       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="52.644µs"
I0805 14:35:39.629865       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="39.545µs"
I0805 14:35:41.630800       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="28.526µs"
I0805 14:35:55.688557       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="66.421µs"
I0805 14:36:11.687621       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="63.426µs"
I0805 14:36:26.684620       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="34.143µs"
I0805 14:36:40.676837       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="287.741µs"
I0805 14:36:51.684917       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/streamlit-dynamodb-deployment-5b8dcc59cd" duration="25.659µs"


==> kube-proxy [33ef4f95e1f4] <==
I0805 14:34:25.105991       1 server_linux.go:69] "Using iptables proxy"
I0805 14:34:25.123781       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0805 14:34:25.218148       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0805 14:34:25.218305       1 server_linux.go:165] "Using iptables Proxier"
I0805 14:34:25.220919       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0805 14:34:25.220963       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0805 14:34:25.221260       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0805 14:34:25.221858       1 server.go:872] "Version info" version="v1.30.0"
I0805 14:34:25.221907       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 14:34:25.223212       1 config.go:192] "Starting service config controller"
I0805 14:34:25.223465       1 config.go:101] "Starting endpoint slice config controller"
I0805 14:34:25.223495       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0805 14:34:25.223527       1 shared_informer.go:313] Waiting for caches to sync for service config
I0805 14:34:25.223588       1 config.go:319] "Starting node config controller"
I0805 14:34:25.223648       1 shared_informer.go:313] Waiting for caches to sync for node config
I0805 14:34:25.323844       1 shared_informer.go:320] Caches are synced for node config
I0805 14:34:25.323856       1 shared_informer.go:320] Caches are synced for service config
I0805 14:34:25.323869       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [4b4af4edffcf] <==
I0803 02:13:19.752765       1 server_linux.go:69] "Using iptables proxy"
I0803 02:13:19.757273       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0803 02:13:19.771822       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0803 02:13:19.771860       1 server_linux.go:165] "Using iptables Proxier"
I0803 02:13:19.772751       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0803 02:13:19.772768       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0803 02:13:19.772807       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0803 02:13:19.772968       1 server.go:872] "Version info" version="v1.30.0"
I0803 02:13:19.772989       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0803 02:13:19.773780       1 config.go:319] "Starting node config controller"
I0803 02:13:19.773828       1 shared_informer.go:313] Waiting for caches to sync for node config
I0803 02:13:19.773780       1 config.go:101] "Starting endpoint slice config controller"
I0803 02:13:19.773882       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0803 02:13:19.773788       1 config.go:192] "Starting service config controller"
I0803 02:13:19.773907       1 shared_informer.go:313] Waiting for caches to sync for service config
I0803 02:13:19.874005       1 shared_informer.go:320] Caches are synced for service config
I0803 02:13:19.874012       1 shared_informer.go:320] Caches are synced for node config
I0803 02:13:19.874028       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [2c551487b745] <==
I0805 14:34:20.338559       1 serving.go:380] Generated self-signed cert in-memory
W0805 14:34:21.925073       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0805 14:34:21.925157       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found, role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found]
W0805 14:34:21.925185       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0805 14:34:21.925206       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0805 14:34:22.301480       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0805 14:34:22.301504       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 14:34:22.304652       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0805 14:34:22.304799       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0805 14:34:22.304860       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0805 14:34:22.304817       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0805 14:34:22.405561       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [ffa3bc8e1ad7] <==
I0803 02:13:02.760099       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0803 02:13:02.760126       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0803 02:13:02.760129       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0803 02:13:02.760154       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0803 02:13:02.761861       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0803 02:13:02.761995       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0803 02:13:02.762017       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0803 02:13:02.762037       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0803 02:13:02.851864       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0803 02:13:02.852200       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0803 02:13:02.852408       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0803 02:13:02.852479       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0803 02:13:02.852543       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0803 02:13:02.852594       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0803 02:13:02.852633       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0803 02:13:02.852615       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0803 02:13:02.852723       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0803 02:13:02.852708       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0803 02:13:02.852807       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0803 02:13:02.852842       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0803 02:13:02.852876       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0803 02:13:02.852908       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0803 02:13:02.852924       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0803 02:13:02.852926       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0803 02:13:02.852950       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0803 02:13:02.852950       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0803 02:13:02.852988       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0803 02:13:02.853056       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0803 02:13:02.853073       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0803 02:13:02.852965       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0803 02:13:02.853145       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0803 02:13:02.853407       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0803 02:13:02.853411       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0803 02:13:02.853467       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0803 02:13:03.669246       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0803 02:13:03.669277       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0803 02:13:03.724087       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0803 02:13:03.724116       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0803 02:13:03.793302       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0803 02:13:03.793337       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0803 02:13:03.857268       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0803 02:13:03.857301       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0803 02:13:03.916323       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0803 02:13:03.916352       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0803 02:13:04.033465       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0803 02:13:04.033494       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0803 02:13:04.069410       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0803 02:13:04.069439       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0803 02:13:04.138147       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0803 02:13:04.138178       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0803 02:13:04.202568       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0803 02:13:04.202595       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0803 02:13:04.270438       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0803 02:13:04.270465       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0803 02:13:04.359652       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0803 02:13:04.359682       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0803 02:13:07.260889       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0803 02:24:40.031829       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0803 02:24:40.031969       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0803 02:24:40.032011       1 run.go:74] "command failed" err="finished without leader elect"


==> kubelet <==
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.707266    1468 topology_manager.go:215] "Topology Admit Handler" podUID="bf018aa9-0995-4afa-a141-2311ca1fd623" podNamespace="kube-system" podName="kube-proxy-25kz8"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.707331    1468 topology_manager.go:215] "Topology Admit Handler" podUID="7a11cf4d-62d9-419d-b636-3e462d318480" podNamespace="kube-system" podName="coredns-7db6d8ff4d-k9dkh"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.707379    1468 topology_manager.go:215] "Topology Admit Handler" podUID="249f9238-cfc2-433b-9027-ef0d15839b7d" podNamespace="kubernetes-dashboard" podName="dashboard-metrics-scraper-b5fc48f67-mt69p"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.707726    1468 topology_manager.go:215] "Topology Admit Handler" podUID="2832e4fa-bb99-4bc0-a690-0ddc2323a836" podNamespace="kubernetes-dashboard" podName="kubernetes-dashboard-779776cb65-rf66d"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.707787    1468 topology_manager.go:215] "Topology Admit Handler" podUID="c4404b22-9db0-4208-a18e-927d70041ac7" podNamespace="kube-system" podName="metrics-server-c59844bb4-rd756"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.760408    1468 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.812021    1468 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/bf018aa9-0995-4afa-a141-2311ca1fd623-lib-modules\") pod \"kube-proxy-25kz8\" (UID: \"bf018aa9-0995-4afa-a141-2311ca1fd623\") " pod="kube-system/kube-proxy-25kz8"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.812150    1468 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/5a24710a-8c1e-4212-aebb-f4b7f7432dde-tmp\") pod \"storage-provisioner\" (UID: \"5a24710a-8c1e-4212-aebb-f4b7f7432dde\") " pod="kube-system/storage-provisioner"
Aug 05 14:34:23 minikube kubelet[1468]: I0805 14:34:23.812216    1468 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/bf018aa9-0995-4afa-a141-2311ca1fd623-xtables-lock\") pod \"kube-proxy-25kz8\" (UID: \"bf018aa9-0995-4afa-a141-2311ca1fd623\") " pod="kube-system/kube-proxy-25kz8"
Aug 05 14:34:24 minikube kubelet[1468]: I0805 14:34:24.824786    1468 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="40bef01fd9a44a5c95c789e33ecf0f3bdb8f2deb7198638782d2181f50de65ef"
Aug 05 14:34:24 minikube kubelet[1468]: I0805 14:34:24.900987    1468 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="07195539f32807a973216bc34363a062600ca5485a461808d183419ea98816ba"
Aug 05 14:34:24 minikube kubelet[1468]: I0805 14:34:24.911092    1468 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="dbfa6efc227aae4c88c4c9589e74aa40b018ab1a299951736a9f1903e5b7024d"
Aug 05 14:34:24 minikube kubelet[1468]: I0805 14:34:24.917386    1468 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="dd3ebb1a4b975a611ff99ffd2ef13c56c8bf87d9fe472edc95fa82389e48c98b"
Aug 05 14:34:24 minikube kubelet[1468]: I0805 14:34:24.998885    1468 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a8f69c87449ef1b30f633b100ac50e1d6dcd9c035f4ac924b53e6c23fbddeaef"
Aug 05 14:34:25 minikube kubelet[1468]: I0805 14:34:25.006282    1468 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="34787c1c3ee5fa84a0853dd44789f1d640720b28aeb3f2e5c660cbbb12280cda"
Aug 05 14:34:25 minikube kubelet[1468]: E0805 14:34:25.829361    1468 summary_sys_containers.go:83] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 14:34:27 minikube kubelet[1468]: I0805 14:34:27.069111    1468 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Aug 05 14:34:28 minikube kubelet[1468]: E0805 14:34:28.723221    1468 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 14:34:28 minikube kubelet[1468]: E0805 14:34:28.723653    1468 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Aug 05 14:34:35 minikube kubelet[1468]: I0805 14:34:35.144130    1468 scope.go:117] "RemoveContainer" containerID="21331c4d29804c6ef5a02d62c4199eb5097d677f1578b3fe6375724fd4a5a48e"
Aug 05 14:34:35 minikube kubelet[1468]: I0805 14:34:35.144223    1468 scope.go:117] "RemoveContainer" containerID="da88bf295aa77196022537d4c26cd009844adaa10f480ec6d6b11d835ab8f2a6"
Aug 05 14:34:35 minikube kubelet[1468]: E0805 14:34:35.144323    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(5a24710a-8c1e-4212-aebb-f4b7f7432dde)\"" pod="kube-system/storage-provisioner" podUID="5a24710a-8c1e-4212-aebb-f4b7f7432dde"
Aug 05 14:34:36 minikube kubelet[1468]: I0805 14:34:36.157821    1468 scope.go:117] "RemoveContainer" containerID="6bad89accb3a735cd38666578c8fd79edc7e201ebd4ee29dc7c9ae0af5840359"
Aug 05 14:34:36 minikube kubelet[1468]: I0805 14:34:36.158031    1468 scope.go:117] "RemoveContainer" containerID="f73e5f493d288cc321a5a7f10974643d7780754f672b2ab183c11907e26119e8"
Aug 05 14:34:36 minikube kubelet[1468]: E0805 14:34:36.158182    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-779776cb65-rf66d_kubernetes-dashboard(2832e4fa-bb99-4bc0-a690-0ddc2323a836)\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-rf66d" podUID="2832e4fa-bb99-4bc0-a690-0ddc2323a836"
Aug 05 14:34:38 minikube kubelet[1468]: E0805 14:34:38.733647    1468 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 14:34:38 minikube kubelet[1468]: E0805 14:34:38.733691    1468 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Aug 05 14:34:43 minikube kubelet[1468]: I0805 14:34:43.091668    1468 scope.go:117] "RemoveContainer" containerID="f73e5f493d288cc321a5a7f10974643d7780754f672b2ab183c11907e26119e8"
Aug 05 14:34:43 minikube kubelet[1468]: E0805 14:34:43.091841    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-779776cb65-rf66d_kubernetes-dashboard(2832e4fa-bb99-4bc0-a690-0ddc2323a836)\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-rf66d" podUID="2832e4fa-bb99-4bc0-a690-0ddc2323a836"
Aug 05 14:34:48 minikube kubelet[1468]: I0805 14:34:48.670072    1468 scope.go:117] "RemoveContainer" containerID="da88bf295aa77196022537d4c26cd009844adaa10f480ec6d6b11d835ab8f2a6"
Aug 05 14:34:48 minikube kubelet[1468]: E0805 14:34:48.749550    1468 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 14:34:48 minikube kubelet[1468]: E0805 14:34:48.749577    1468 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Aug 05 14:34:54 minikube kubelet[1468]: I0805 14:34:54.670363    1468 scope.go:117] "RemoveContainer" containerID="f73e5f493d288cc321a5a7f10974643d7780754f672b2ab183c11907e26119e8"
Aug 05 14:34:58 minikube kubelet[1468]: E0805 14:34:58.768663    1468 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 14:34:58 minikube kubelet[1468]: E0805 14:34:58.768695    1468 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Aug 05 14:35:08 minikube kubelet[1468]: E0805 14:35:08.784036    1468 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 14:35:08 minikube kubelet[1468]: E0805 14:35:08.784072    1468 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Aug 05 14:35:10 minikube kubelet[1468]: I0805 14:35:10.781446    1468 topology_manager.go:215] "Topology Admit Handler" podUID="efde963e-4b40-486a-b0d7-ed1f1255b642" podNamespace="default" podName="dynamodb-local-57d4f8ccd9-t9x56"
Aug 05 14:35:10 minikube kubelet[1468]: I0805 14:35:10.828739    1468 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-g98rq\" (UniqueName: \"kubernetes.io/projected/efde963e-4b40-486a-b0d7-ed1f1255b642-kube-api-access-g98rq\") pod \"dynamodb-local-57d4f8ccd9-t9x56\" (UID: \"efde963e-4b40-486a-b0d7-ed1f1255b642\") " pod="default/dynamodb-local-57d4f8ccd9-t9x56"
Aug 05 14:35:10 minikube kubelet[1468]: I0805 14:35:10.828781    1468 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"dynamodb-data\" (UniqueName: \"kubernetes.io/empty-dir/efde963e-4b40-486a-b0d7-ed1f1255b642-dynamodb-data\") pod \"dynamodb-local-57d4f8ccd9-t9x56\" (UID: \"efde963e-4b40-486a-b0d7-ed1f1255b642\") " pod="default/dynamodb-local-57d4f8ccd9-t9x56"
Aug 05 14:35:38 minikube kubelet[1468]: I0805 14:35:38.612241    1468 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/dynamodb-local-57d4f8ccd9-t9x56" podStartSLOduration=1.8352466939999998 podStartE2EDuration="28.609928483s" podCreationTimestamp="2024-08-05 14:35:10 +0000 UTC" firstStartedPulling="2024-08-05 14:35:11.469835823 +0000 UTC m=+52.949517136" lastFinishedPulling="2024-08-05 14:35:38.244517611 +0000 UTC m=+79.724198925" observedRunningTime="2024-08-05 14:35:38.608328041 +0000 UTC m=+80.088009358" watchObservedRunningTime="2024-08-05 14:35:38.609928483 +0000 UTC m=+80.089609800"
Aug 05 14:35:39 minikube kubelet[1468]: I0805 14:35:39.618956    1468 topology_manager.go:215] "Topology Admit Handler" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698" podNamespace="default" podName="streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz"
Aug 05 14:35:39 minikube kubelet[1468]: I0805 14:35:39.710286    1468 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jjmpf\" (UniqueName: \"kubernetes.io/projected/3838705f-ee75-4e6e-a83c-5dc948df2698-kube-api-access-jjmpf\") pod \"streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz\" (UID: \"3838705f-ee75-4e6e-a83c-5dc948df2698\") " pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz"
Aug 05 14:35:41 minikube kubelet[1468]: E0805 14:35:41.207466    1468 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="streamlit-dynamodb-app:latest"
Aug 05 14:35:41 minikube kubelet[1468]: E0805 14:35:41.207969    1468 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="streamlit-dynamodb-app:latest"
Aug 05 14:35:41 minikube kubelet[1468]: E0805 14:35:41.208885    1468 kuberuntime_manager.go:1256] container &Container{Name:streamlit-dynamodb,Image:streamlit-dynamodb-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8501,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjmpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz_default(3838705f-ee75-4e6e-a83c-5dc948df2698): ErrImagePull: Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 14:35:41 minikube kubelet[1468]: E0805 14:35:41.209253    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ErrImagePull: \"Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"
Aug 05 14:35:41 minikube kubelet[1468]: E0805 14:35:41.621862    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ImagePullBackOff: \"Back-off pulling image \\\"streamlit-dynamodb-app:latest\\\"\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"
Aug 05 14:35:56 minikube kubelet[1468]: E0805 14:35:56.385840    1468 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="streamlit-dynamodb-app:latest"
Aug 05 14:35:56 minikube kubelet[1468]: E0805 14:35:56.385903    1468 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="streamlit-dynamodb-app:latest"
Aug 05 14:35:56 minikube kubelet[1468]: E0805 14:35:56.386009    1468 kuberuntime_manager.go:1256] container &Container{Name:streamlit-dynamodb,Image:streamlit-dynamodb-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8501,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjmpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz_default(3838705f-ee75-4e6e-a83c-5dc948df2698): ErrImagePull: Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 14:35:56 minikube kubelet[1468]: E0805 14:35:56.386031    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ErrImagePull: \"Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"
Aug 05 14:36:11 minikube kubelet[1468]: E0805 14:36:11.674503    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ImagePullBackOff: \"Back-off pulling image \\\"streamlit-dynamodb-app:latest\\\"\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"
Aug 05 14:36:27 minikube kubelet[1468]: E0805 14:36:27.470923    1468 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="streamlit-dynamodb-app:latest"
Aug 05 14:36:27 minikube kubelet[1468]: E0805 14:36:27.470993    1468 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="streamlit-dynamodb-app:latest"
Aug 05 14:36:27 minikube kubelet[1468]: E0805 14:36:27.471128    1468 kuberuntime_manager.go:1256] container &Container{Name:streamlit-dynamodb,Image:streamlit-dynamodb-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8501,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjmpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz_default(3838705f-ee75-4e6e-a83c-5dc948df2698): ErrImagePull: Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 14:36:27 minikube kubelet[1468]: E0805 14:36:27.471158    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ErrImagePull: \"Error response from daemon: pull access denied for streamlit-dynamodb-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"
Aug 05 14:36:40 minikube kubelet[1468]: E0805 14:36:40.671335    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ImagePullBackOff: \"Back-off pulling image \\\"streamlit-dynamodb-app:latest\\\"\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"
Aug 05 14:36:51 minikube kubelet[1468]: E0805 14:36:51.674573    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ImagePullBackOff: \"Back-off pulling image \\\"streamlit-dynamodb-app:latest\\\"\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"
Aug 05 14:37:05 minikube kubelet[1468]: E0805 14:37:05.673311    1468 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"streamlit-dynamodb\" with ImagePullBackOff: \"Back-off pulling image \\\"streamlit-dynamodb-app:latest\\\"\"" pod="default/streamlit-dynamodb-deployment-5b8dcc59cd-ddbqz" podUID="3838705f-ee75-4e6e-a83c-5dc948df2698"


==> kubernetes-dashboard [774a9b74dbfa] <==
2024/08/05 14:34:54 Starting overwatch
2024/08/05 14:34:54 Using namespace: kubernetes-dashboard
2024/08/05 14:34:54 Using in-cluster config to connect to apiserver
2024/08/05 14:34:54 Using secret token for csrf signing
2024/08/05 14:34:54 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/08/05 14:34:54 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/08/05 14:34:54 Successful initial request to the apiserver, version: v1.30.0
2024/08/05 14:34:54 Generating JWE encryption key
2024/08/05 14:34:54 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/08/05 14:34:54 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/08/05 14:34:54 Initializing JWE encryption key from synchronized object
2024/08/05 14:34:54 Creating in-cluster Sidecar client
2024/08/05 14:34:54 Successful request to sidecar
2024/08/05 14:34:54 Serving insecurely on HTTP port: 9090


==> kubernetes-dashboard [f73e5f493d28] <==
2024/08/05 14:34:25 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": net/http: TLS handshake timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00089fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc0001e6080)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2024/08/05 14:34:25 Using namespace: kubernetes-dashboard
2024/08/05 14:34:25 Using in-cluster config to connect to apiserver
2024/08/05 14:34:25 Using secret token for csrf signing
2024/08/05 14:34:25 Initializing csrf token from kubernetes-dashboard-csrf secret


==> storage-provisioner [a66d1a519d72] <==
I0805 14:34:48.746943       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0805 14:34:48.754509       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0805 14:34:48.755137       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0805 14:35:06.164781       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0805 14:35:06.164911       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_27db54d6-ef3c-4d8d-a222-57497ccfbaa6!
I0805 14:35:06.164895       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"652f3f3f-cfda-4890-8820-3aecf8348078", APIVersion:"v1", ResourceVersion:"1251", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_27db54d6-ef3c-4d8d-a222-57497ccfbaa6 became leader
I0805 14:35:06.267120       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_27db54d6-ef3c-4d8d-a222-57497ccfbaa6!


==> storage-provisioner [da88bf295aa7] <==
I0805 14:34:25.010312       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0805 14:34:35.021382       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

